# DORSZ - DokÅ‚adne Odpytywanie RozpoznajÄ…ce Sedno Zagadnienia

<p align="center">
<img src="img/dorsz.png" alt="Alt text" width="500">
</p>

Uniwersalny runtime dla wielu agentÃ³w AI (obecnie: Why5, Ishikawa oraz Temperature-Check) korzystajÄ…cych z interfejsu OpenAI-compatible. Projekt jest nastawiony na dewelopera Pythona: uÅ¼ywa uv do zarzÄ…dzania Å›rodowiskiem i zaleÅ¼noÅ›ciami, a uruchamianie odbywa siÄ™ przez `uv run`.

## Szybki start

1) Wymagania:
- Python >= 3.13
- [uv](https://github.com/astral-sh/uv)
- Uruchomiony provider LLM: lokalny endpoint OpenAI-compatible (np. LMS Studio) lub OpenAI

2) Instalacja uv:
- macOS (Homebrew):
```bash
brew install uv
```
- Skrypt instalacyjny (macOS/Linux):
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

3) Klon repozytorium i instalacja zaleÅ¼noÅ›ci:
```bash
git clone <repo-url>
cd dorsz
uv sync
```

4) Uruchom pierwszy agent (przykÅ‚ad z LMS Studio i modelem Bielik):
```bash
uv run python main.py why5 --provider local --model bielik-11b-v2.6-instruct
```

Uwaga:
- Dla lokalnego endpointu (np. LMS Studio) wÅ‚Ä…cz OpenAI-compatible API na `http://localhost:1234/v1`.
- Dla OpenAI ustaw `OPENAI_API_KEY` w Å›rodowisku lub pliku `.env`.

---

## Uruchamianie â€” skÅ‚adnia

OgÃ³lny wzorzec:
```bash
uv run python main.py <agent_id> --provider <local|openai> [--model <nazwa_modelu>]
```

Parametry:
- `agent_id` (wymagany): `why5` | `ishikawa` | `temperature_check`
- `--provider` (opcjonalny): `local` (domyÅ›lny), `openai`
- `--model` (opcjonalny): jeÅ›li pominiÄ™ty, uÅ¼yje ENV `MODEL` lub domyÅ›lnego; nazwa modelu zgodna z wybranym providerem

PrzykÅ‚ady (z uÅ¼yciem modelu Bielik oraz innych):

- Why5:
```bash
# Lokalny endpoint + Bielik
uv run python main.py why5 --provider local --model bielik-11b-v2.6-instruct


# OpenAI (wymaga OPENAI_API_KEY)
uv run python main.py why5 --provider openai --model gpt-4o
```

- Ishikawa:
```bash
# Lokalny endpoint + Bielik
uv run python main.py ishikawa --provider local --model bielik-11b-v2.6-instruct


# OpenAI (wymaga OPENAI_API_KEY)
uv run python main.py ishikawa --provider openai --model gpt-4o
```

- Temperature-Check:
```bash
# Lokalny endpoint + Bielik
uv run python main.py temperature_check --provider local --model bielik-11b-v2.6-instruct


# OpenAI (wymaga OPENAI_API_KEY)
uv run python main.py temperature_check --provider openai --model gpt-4o
```

WskazÃ³wka: domyÅ›lne wejÅ›cie poczÄ…tkowe dla kaÅ¼dego agenta jest ustawione w `main.py` w `AGENT_DEFAULT_INPUTS`.

---

## Agenci

Projekt zawiera kilka agentÃ³w. Opis kaÅ¼dego z nich znajduje siÄ™ w dedykowanym podrozdziale.

### 1) Agent Why5

Asystent AI do szybkiej analizy przyczyn ÅºrÃ³dÅ‚owych przy uÅ¼yciu techniki â€5 x Dlaczegoâ€ (5 Whys).

Jak dziaÅ‚a:
1. Agent rozpoczyna rozmowÄ™, pytajÄ…c o problem do przeanalizowania (uÅ¼ywa narzÄ™dzia `ask_human`).
2. Zadaje kolejne pytania â€Dlaczego?â€, maksymalnie 5 razy w jednym Å‚aÅ„cuchu.
3. KoÅ„czy, gdy dotrze do przyczyny, ktÃ³rÄ… moÅ¼na bezpoÅ›rednio zaadresowaÄ‡ dziaÅ‚aniem, albo gdy dalsze pytania nie wnoszÄ… nowych informacji.
4. Generuje strukturalne podsumowanie: sformuÅ‚owanie problemu, Å‚aÅ„cuch â€Dlaczego?â€, gÅ‚Ã³wne przyczyny ÅºrÃ³dÅ‚owe, dziaÅ‚ania naprawcze i wnioski (model `Why5Summary` w `agents_why5.py`).

PrzykÅ‚ad uruchomienia z Bielik:
```bash
uv run python main.py why5 --provider local --model bielik-11b-v2.6-instruct
```

PrzykÅ‚adowa sesja (fragment):
```
DORSZ - DokÅ‚adne Odpytywanie RozpoznajÄ…ce Sedno Zagadnienia
Provider: local
Model: bielik-11b-v2.6-instruct
Agent: why5
Wpisz swÃ³j problem lub pytanie, aby rozpoczÄ…Ä‡ analizÄ™.

ğŸ¤– Agent: CzeÅ›Ä‡! ChÄ™tnie pomogÄ™ Ci przeanalizowaÄ‡ jakiÅ› problem metodÄ… "5 x Dlaczego".
Jaki problem lub sytuacjÄ™ chciaÅ‚byÅ› przeanalizowaÄ‡?

ğŸ¤” Agent pytanie: Jaki problem chciaÅ‚byÅ› przeanalizowaÄ‡?
ğŸ‘¤ Twoja odpowiedÅº: ZespÃ³Å‚ nie dotrzymuje deadlineÃ³w w projektach

[... dalsza interaktywna rozmowa ...]

================================================================================
ğŸ¦… PODSUMOWANIE ANALIZY '5 x DLACZEGO'
================================================================================
ğŸ“‹ Problem: ZespÃ³Å‚ regularnie nie dotrzymuje deadlineÃ³w w projektach...
â“ ÅaÅ„cuch "Dlaczego?" (...)
ğŸ” NajwaÅ¼niejsze przyczyny ÅºrÃ³dÅ‚owe (...)
âš¡ DziaÅ‚ania naprawcze (...)
ğŸ’¡ Kluczowe wnioski (...)
================================================================================
```

### 2) Agent Ishikawa

Asystent AI do analizy przyczyn ÅºrÃ³dÅ‚owych z uÅ¼yciem diagramu Ishikawy (rybia oÅ›Ä‡, 5M+E), z poprawnymi kategoriami:
- CzÅ‚owiek (Man)
- Maszyna (Machine)
- MateriaÅ‚ (Material)
- Metoda (Method)
- Pomiary (Measurement)
- Åšrodowisko (Environment)

Jak dziaÅ‚a:
1. Agent rozpoczyna rozmowÄ™, pytajÄ…c o problem do przeanalizowania (uÅ¼ywa narzÄ™dzia `ask_human`).
2. Systematycznie przechodzi przez kategorie 5M+E i zbiera przyczyny, opcjonalnie dopytujÄ…c â€Dlaczego?â€ w ramach kategorii.
3. Korzysta z globalnej listy tematÃ³w (`add_topic`, `mark_topic_answered`, `next_unanswered_topic`, `get_topics_summary`), Å¼eby parkowaÄ‡ poboczne wÄ…tki.
4. Generuje strukturalne podsumowanie: sformuÅ‚owanie problemu, listÄ™ przyczyn z kategoriami Ishikawy, dziaÅ‚ania naprawcze, kluczowe wnioski (model `IshikawaSummary` w `agents_ishikawa.py`).

PrzykÅ‚ad uruchomienia z Bielik:
```bash
uv run python main.py ishikawa --provider local --model bielik-11b-v2.6-instruct
```

### 3) Agent Temperature-Check

Prosty agent testowy, prezentujÄ…cy wywoÅ‚anie narzÄ™dzia i zwracanie wynikÃ³w w strukturze.

Zasady dziaÅ‚ania:
- Ma dedykowane narzÄ™dzie `get_temperature(place)`, ktÃ³re w tym scenariuszu zwraca staÅ‚Ä… odpowiedÅº (do demonstracji integracji tool-call).
- Zawsze wywoÅ‚uje narzÄ™dzie dokÅ‚adnie raz.
- Zwraca wynik jako strukturÄ™ `TemperatureReport` (Pydantic) z polami: miejsce, temperatura (Â°C/Â°F), warunki.

DomyÅ›lne wejÅ›cie:
- JeÅ›li nie podano miejsca, wykorzystywane jest â€Warszawaâ€.
- W aktualnym runtime wejÅ›cie poczÄ…tkowe jest dostarczane z `AGENT_DEFAULT_INPUTS` (patrz `main.py`). Aby zmieniÄ‡ domyÅ›lne miejsce startowe, zaktualizuj `AGENT_DEFAULT_INPUTS["temperature_check"]`.

PrzykÅ‚ad uruchomienia z Bielik:
```bash
uv run python main.py temperature_check --provider local --model bielik-11b-v2.6-instruct
```

---

## Konfiguracja providerÃ³w

DomyÅ›lne adresy bazowe (zdefiniowane w `main.py`):
- Lokalny endpoint (np. LMS Studio): `http://localhost:1234/v1`
- OpenAI: uÅ¼ywa domyÅ›lnych ustawieÅ„ biblioteki OpenAI (wymaga `OPENAI_API_KEY`)

Parameter `--provider` przyjmuje wartoÅ›ci: `local` (domyÅ›lny), `openai`.

---

## Struktura kodu

- `main.py` â€” ogÃ³lny runtime:
  - Rejestr agentÃ³w, fabryki (`AGENT_FACTORIES`) i renderery (`AGENT_RENDERERS`)
  - DomyÅ›lne wejÅ›cia startowe (`AGENT_DEFAULT_INPUTS`)
  - Konfiguracja providerÃ³w i klienta OpenAI-compatible
  - PÄ™tla uruchomieniowa i drukowanie wynikÃ³w
- `agents_why5.py` â€” definicja agenta Why5:
  - Instrukcje (prompt), narzÄ™dzie `ask_human`, model Pydantic `Why5Summary`, renderer podsumowania
- `agents_ishikawa.py` â€” definicja agenta Ishikawa:
  - Instrukcje (prompt), narzÄ™dzie `ask_human`, narzÄ™dzia do zarzÄ…dzania tematami, modele Pydantic `Ishikawa*`, renderer podsumowania
- `agents_temperature_check.py` â€” definicja agenta Temperature-Check:
  - NarzÄ™dzie `get_temperature`, struktura `TemperatureReport`, renderer

---

## Konfiguracja (.env)

Plik `.env` (opcjonalny), wczytywany automatycznie:
```bash
# Default model (used if --model is omitted)
MODEL=Bielik-4.5B-v3.0-Instruct.Q8_0.gguf

# OpenAI (cloud):
OPENAI_API_KEY=...

# Local (OpenAI-compatible) endpoint:
LOCAL_BASE_URL=http://localhost:1234/v1
LOCAL_API_KEY=EMPTY

# Opcjonalnie: inne ustawienia Å›rodowiskowe
```

---

## Testy

Aby uruchomiÄ‡ testy integracyjne:

- Wymagania: uruchomiony lokalny serwer zgodny z OpenAI API pod adresem `http://localhost:1234/v1`. JeÅ›li serwer nie jest osiÄ…galny, testy zostanÄ… pominiÄ™te.
- Model: testy uÅ¼ywajÄ… modelu w kolejnoÅ›ci: 1) parametr `--model`, 2) zmienna Å›rodowiskowa `MODEL`, 3) domyÅ›lny z `.env.example`.
- Klucze/sekrety: `OPENAI_API_KEY` nie jest wymagany; testy ustawiajÄ… go automatycznie na `EMPTY`.

PrzykÅ‚ady:
```bash
uv run pytest
uv run pytest -q
uv run pytest -q --model Bielik-4.5B-v3.0-Instruct.Q8_0.gguf
```

---

## Temperatura modeli

- Why5: domyÅ›lna temperatura `0.1` (patrz `agents_why5.py`)
- Ishikawa: domyÅ›lna temperatura `0.1` (patrz `agents_ishikawa.py`)
- Temperature-Check: domyÅ›lna temperatura `0.0` (patrz `agents_temperature_check.py`)

---

## Dodatkowe uwagi

- Parametr `--model` jest opcjonalny; jeÅ›li pominiÄ™ty, uÅ¼yty zostanie `MODEL` z env lub domyÅ›lny.
- Dla lokalnego endpointu (np. LMS Studio) wymagane jest uruchomienie serwera zgodnego z OpenAI API pod wskazanym adresem.

---

## Inferencja modelu Bielik (lokalnie, GGUF)

PoniÅ¼ej opisano, jak uruchomiÄ‡ inferencjÄ™ lokalnego modelu oraz jak wpiÄ…Ä‡ go do DORSZ jako lokalny provider.

### Krok 1: Pobierz model GGUF

- PrzejdÅº do repozytorium na Hugging Face, np.:
  https://huggingface.co/speakleash/Bielik-4.5B-v3.0-Instruct-GGUF/tree/main
- Pobierz wariant w oczekiwanej kwantyzacji (np. Q8_0) lub model bez kwantyzacji.
- UmieÅ›Ä‡ plik w lokalnym katalogu, np. `./Bielik-4.5B-v3.0-Instruct-GGUF/`.

PrzykÅ‚adowy plik: `Bielik-4.5B-v3.0-Instruct.Q8_0.gguf`

### Opcja A: CLI (llama.cpp)

1) Instalacja llama.cpp:
- macOS (Homebrew):
```bash
brew install llama.cpp
```
- Alternatywnie z kodu ÅºrÃ³dÅ‚owego: https://github.com/ggml-org/llama.cpp

2) Uruchomienie serwera OpenAI-compatible:

Z Bielikiem v2.6
```bash
llama-server --port 1234 -c 32768 -m ./Bielik-11B-v2.6-Instruct-GGUF/Bielik-11B-v2.6-Instruct.Q8_0.gguf
```

Z Bielikiem v3.0
```bash
llama-server --port 1234 -m ./Bielik-4.5B-v3.0-Instruct-GGUF/Bielik-4.5B-v3.0-Instruct.Q8_0.gguf
```

MoÅ¼na rÃ³wnieÅ¼ podaÄ‡ parametr `-c` okreÅ›lajÄ…cy rozmiar kontekstu w bajtach (zajrzyj do karty modelu na HF aby sprawdziÄ‡ dla jakiego rozmiaru kontekstu byÅ‚ trenowany model).
Serwer bÄ™dzie dostÄ™pny pod adresem `http://localhost:1234/v1`.
Server nie wymaga podania nazwy modelu przy wywoÅ‚aniach API, ale dla spujnoÅ›ci moÅ¼emy go uÅ¼ywaÄ‡. 

3) Integracja z DORSZ:
- Upewnij siÄ™, Å¼e `LOCAL_BASE_URL=http://localhost:1234/v1` (np. w `.env`).
- Uruchom agenta, wskazujÄ…c nazwÄ™ modelu (tu: nazwÄ™ pliku GGUF):
```bash
uv run python main.py why5 --provider local --model Bielik-4.5B-v3.0-Instruct.Q8_0.gguf
```
Analogicznie moÅ¼esz uruchomiÄ‡ `ishikawa` oraz `temperature_check`.

### Opcja B: UI (Jan)

JeÅ›li wolisz interfejs graficzny, skorzystaj z projektu Jan:
- Repozytorium: https://github.com/janhq/jan
- Kroki:
  1. Zainstaluj aplikacjÄ™ (patrz â€Releasesâ€ na GitHub).
  2. Dodaj model, wskazujÄ…c pobrany plik `.gguf` (Import/Local model).
  3. Rozmawiaj z modelem bezpoÅ›rednio w UI.
  4. (Opcjonalnie) JeÅ›li w Jan dostÄ™pne jest lokalne API zgodne z OpenAI, wÅ‚Ä…cz je i wskaÅ¼ jego adres w `LOCAL_BASE_URL`, aby uÅ¼ywaÄ‡ Jana jako providera dla DORSZ.

Uwagi:
- W przypadku ograniczeÅ„ pamiÄ™ci wybierz lÅ¼ejszÄ… kwantyzacjÄ™ (np. Q6_K, Q5_K_M). Wersja Q8_0 to dobry kompromis jakoÅ›ci/szybkoÅ›ci na CPU.
- MoÅ¼esz ustawiÄ‡ domyÅ›lny model w `.env` (zmienna `MODEL`), a nastÄ™pnie pominÄ…Ä‡ parametr `--model` przy uruchamianiu.
