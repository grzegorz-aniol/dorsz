# DORSZ - DokÅ‚adne Odpytywanie RozpoznajÄ…ce Sedno Zagadnienia

<p align="center">
<img src="img/dorsz.png" alt="Alt text" width="500">
</p>

Uniwersalny runtime dla wielu agentÃ³w AI (obecnie: Why5, Ishikawa oraz Temperature-Check) korzystajÄ…cych z interfejsu OpenAI-compatible. Projekt jest nastawiony na dewelopera Pythona: uÅ¼ywa uv do zarzÄ…dzania Å›rodowiskiem i zaleÅ¼noÅ›ciami, a uruchamianie odbywa siÄ™ przez `uv run`.

## Szybki start

1) Wymagania:
- Python >= 3.13
- [uv](https://github.com/astral-sh/uv)
- Uruchomiony provider LLM: lokalny endpoint OpenAI-compatible (np. `llama-server`) lub OpenAI

2) Instalacja uv:
- macOS (Homebrew):
```bash
brew install uv
```
- Skrypt instalacyjny (macOS/Linux):
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

3) Klon repozytorium i instalacja zaleÅ¼noÅ›ci:
```bash
git clone <repo-url>
cd dorsz
uv sync
```

4) Uruchom pierwszego agenta (przykÅ‚ad z lokalnym `llama-server` i modelem Bielik-11B-v3.0-Instruct):
```bash
uv run python main.py why5 --provider local --model Bielik-11B-v3.0-Instruct
```

Uwaga:
- Lokalny endpoint skonfigurowany jest na `http://localhost:1234/v1`.
- Dla OpenAI ustaw `OPENAI_API_KEY` w Å›rodowisku lub pliku `.env`.

---

## Uruchamianie â€” skÅ‚adnia

OgÃ³lny wzorzec:
```bash
uv run python main.py <agent_id> --provider <local|openai> [--model <nazwa_modelu>]
```

Parametry:
- `agent_id` (wymagany): `why5` | `ishikawa` | `temperature_check`
- `--provider` (opcjonalny): `local` (domyÅ›lny), `openai`
- `--model` (opcjonalny): jeÅ›li pominiÄ™ty, uÅ¼yje ENV `MODEL` lub domyÅ›lnego; nazwa modelu zgodna z wybranym providerem

PrzykÅ‚ady (z uÅ¼yciem modelu Bielik oraz innych):

- Why5:
```bash
# Lokalny endpoint + Bielik-11B-v3.0-Instruct
uv run python main.py why5 --provider local --model Bielik-11B-v3.0-Instruct


# OpenAI (wymaga OPENAI_API_KEY)
uv run python main.py why5 --provider openai --model gpt-4o
```

- Ishikawa:
```bash
# Lokalny endpoint + Bielik-11B-v3.0-Instruct
uv run python main.py ishikawa --provider local --model Bielik-11B-v3.0-Instruct


# OpenAI (wymaga OPENAI_API_KEY)
uv run python main.py ishikawa --provider openai --model gpt-4o
```

- Temperature-Check:
```bash
# Lokalny endpoint + Bielik-11B-v3.0-Instruct
uv run python main.py temperature_check --provider local --model Bielik-11B-v3.0-Instruct


# OpenAI (wymaga OPENAI_API_KEY)
uv run python main.py temperature_check --provider openai --model gpt-4o
```

WskazÃ³wka: domyÅ›lne wejÅ›cie poczÄ…tkowe dla kaÅ¼dego agenta jest ustawione w `main.py` w `AGENT_DEFAULT_INPUTS`.

---

## Agenci

Projekt zawiera kilka agentÃ³w. Opis kaÅ¼dego z nich znajduje siÄ™ w dedykowanym podrozdziale.

### 1) Agent Why5

Asystent AI do szybkiej analizy przyczyn ÅºrÃ³dÅ‚owych przy uÅ¼yciu techniki â€5 x Dlaczegoâ€ (5 Whys).

Jak dziaÅ‚a:
1. Agent rozpoczyna rozmowÄ™, pytajÄ…c o problem do przeanalizowania (uÅ¼ywa narzÄ™dzia `ask_human`).
2. Zadaje kolejne pytania â€Dlaczego?â€, maksymalnie 5 razy w jednym Å‚aÅ„cuchu.
3. KoÅ„czy, gdy dotrze do przyczyny, ktÃ³rÄ… moÅ¼na bezpoÅ›rednio zaadresowaÄ‡ dziaÅ‚aniem, albo gdy dalsze pytania nie wnoszÄ… nowych informacji.
4. Generuje strukturalne podsumowanie: sformuÅ‚owanie problemu, Å‚aÅ„cuch â€Dlaczego?â€, gÅ‚Ã³wne przyczyny ÅºrÃ³dÅ‚owe, dziaÅ‚ania naprawcze i wnioski (model `Why5Summary` w `agents_why5.py`).

PrzykÅ‚ad uruchomienia z Bielik-11B-v3.0-Instruct:
```bash
uv run python main.py why5 --provider local --model Bielik-11B-v3.0-Instruct
```

PrzykÅ‚adowa sesja (fragment):
```
DORSZ - DokÅ‚adne Odpytywanie RozpoznajÄ…ce Sedno Zagadnienia
Provider: local
Model: Bielik-11B-v3.0-Instruct
Agent: why5
Wpisz swÃ³j problem lub pytanie, aby rozpoczÄ…Ä‡ analizÄ™.

ğŸ¤– Agent: CzeÅ›Ä‡! ChÄ™tnie pomogÄ™ Ci przeanalizowaÄ‡ jakiÅ› problem metodÄ… "5 x Dlaczego".
Jaki problem lub sytuacjÄ™ chciaÅ‚byÅ› przeanalizowaÄ‡?

ğŸ¤” Agent pytanie: Jaki problem chciaÅ‚byÅ› przeanalizowaÄ‡?
ğŸ‘¤ Twoja odpowiedÅº: ZespÃ³Å‚ nie dotrzymuje deadlineÃ³w w projektach

[... dalsza interaktywna rozmowa ...]

================================================================================
ğŸ¦… PODSUMOWANIE ANALIZY '5 x DLACZEGO'
================================================================================
ğŸ“‹ Problem: ZespÃ³Å‚ regularnie nie dotrzymuje deadlineÃ³w w projektach...
â“ ÅaÅ„cuch "Dlaczego?" (...)
ğŸ” NajwaÅ¼niejsze przyczyny ÅºrÃ³dÅ‚owe (...)
âš¡ DziaÅ‚ania naprawcze (...)
ğŸ’¡ Kluczowe wnioski (...)
================================================================================
```

### 2) Agent Ishikawa

Asystent AI do analizy przyczyn ÅºrÃ³dÅ‚owych z uÅ¼yciem diagramu Ishikawy (rybia oÅ›Ä‡, 5M+E), z poprawnymi kategoriami:
- CzÅ‚owiek (Man)
- Maszyna (Machine)
- MateriaÅ‚ (Material)
- Metoda (Method)
- Pomiary (Measurement)
- Åšrodowisko (Environment)

Jak dziaÅ‚a:
1. Agent rozpoczyna rozmowÄ™, pytajÄ…c o problem do przeanalizowania (uÅ¼ywa narzÄ™dzia `ask_human`).
2. Systematycznie przechodzi przez kategorie 5M+E i zbiera przyczyny, opcjonalnie dopytujÄ…c â€Dlaczego?â€ w ramach kategorii.
3. Korzysta z globalnej listy tematÃ³w (`add_topic`, `mark_topic_answered`, `next_unanswered_topic`, `get_topics_summary`), Å¼eby parkowaÄ‡ poboczne wÄ…tki.
4. Generuje strukturalne podsumowanie: sformuÅ‚owanie problemu, listÄ™ przyczyn z kategoriami Ishikawy, dziaÅ‚ania naprawcze, kluczowe wnioski (model `IshikawaSummary` w `agents_ishikawa.py`).

PrzykÅ‚ad uruchomienia z Bielik-11B-v3.0-Instruct:
```bash
uv run python main.py ishikawa --provider local --model Bielik-11B-v3.0-Instruct
```

### 3) Agent Temperature-Check

Prosty agent testowy, prezentujÄ…cy wywoÅ‚anie narzÄ™dzia i zwracanie wynikÃ³w w strukturze.

Zasady dziaÅ‚ania:
- Ma dedykowane narzÄ™dzie `get_temperature(place)`, ktÃ³re w tym scenariuszu zwraca staÅ‚Ä… odpowiedÅº (do demonstracji integracji tool-call).
- Zawsze wywoÅ‚uje narzÄ™dzie dokÅ‚adnie raz.
- Zwraca wynik jako strukturÄ™ `TemperatureReport` (Pydantic) z polami: miejsce, temperatura (Â°C/Â°F), warunki.

DomyÅ›lne wejÅ›cie:
- JeÅ›li nie podano miejsca, wykorzystywane jest â€Warszawaâ€.
- W aktualnym runtime wejÅ›cie poczÄ…tkowe jest dostarczane z `AGENT_DEFAULT_INPUTS` (patrz `main.py`). Aby zmieniÄ‡ domyÅ›lne miejsce startowe, zaktualizuj `AGENT_DEFAULT_INPUTS["temperature_check"]`.

PrzykÅ‚ad uruchomienia z Bielik-11B-v3.0-Instruct:
```bash
uv run python main.py temperature_check --provider local --model Bielik-11B-v3.0-Instruct
```

---

## Konfiguracja providerÃ³w

DomyÅ›lne adresy bazowe (zdefiniowane w `main.py`):
- Lokalny endpoint (np. `llama-server`): `http://localhost:1234/v1`
- OpenAI: uÅ¼ywa domyÅ›lnych ustawieÅ„ biblioteki OpenAI (wymaga `OPENAI_API_KEY`)

Parameter `--provider` przyjmuje wartoÅ›ci: `local` (domyÅ›lny), `openai`.

---




## Konfiguracja (.env)

Plik `.env` (opcjonalny), wczytywany automatycznie:
```bash
# Default model (used if --model is omitted)
MODEL=Bielik-11B-v3.0-Instruct

# OpenAI (cloud):
OPENAI_API_KEY=...

# Local (OpenAI-compatible) endpoint:
LOCAL_BASE_URL=http://localhost:1234/v1
LOCAL_API_KEY=EMPTY

# Langfuse (optional observability)
LANGFUSE_SECRET_KEY=sk-..
LANGFUSE_PUBLIC_KEY=pk-..
LANGFUSE_BASE_URL=http://localhost:9300
```

Opis kluczowych zmiennych Å›rodowiskowych:

| Zmienna | Wymagane | Opis |
| --- | --- | --- |
| `MODEL` | Nie (domyÅ›lnie `Bielik-11B-v3.0-Instruct`) | Nazwa modelu przekazywana do providerÃ³w lokalnych; uÅ¼ywana takÅ¼e przez testy, jeÅ›li nie podasz `--model`. |
| `OPENAI_API_KEY` | Tak, gdy korzystasz z OpenAI | Klucz API wymagany do poÅ‚Ä…czenia z chmurÄ… OpenAI. |
| `LOCAL_BASE_URL` | Nie (domyÅ›lnie `http://localhost:1234/v1`) | Adres endpointu OpenAI-compatible uruchomionego lokalnie, np. `llama-server`. |
| `LOCAL_API_KEY` | Nie | Klucz dla lokalnego endpointu (czÄ™sto `EMPTY`, jeÅ›li serwer nie wymaga autoryzacji). |
| `LANGFUSE_SECRET_KEY` / `LANGFUSE_PUBLIC_KEY` / `LANGFUSE_BASE_URL` | Opcjonalnie | Dane dostÄ™pu do Langfuse, jeÅ›li chcesz wysyÅ‚aÄ‡ telemetriÄ™/obserwowalnoÅ›Ä‡. |

PeÅ‚ny zestaw zmiennych wraz z komentarzami znajdziesz w `.env.example`.

---

## Testy

Aby uruchomiÄ‡ testy integracyjne:

- Wymagania: uruchomiony lokalny serwer zgodny z OpenAI API pod adresem `http://localhost:1234/v1`. JeÅ›li serwer nie jest osiÄ…galny, testy zostanÄ… pominiÄ™te.
- Model: testy uÅ¼ywajÄ… modelu w kolejnoÅ›ci: 1) parametr `--model`, 2) zmienna Å›rodowiskowa `MODEL`, 3) domyÅ›lny z `.env.example`.
- Klucze/sekrety: `OPENAI_API_KEY` nie jest wymagany; testy ustawiajÄ… go automatycznie na `EMPTY`.

PrzykÅ‚ady:
```bash
uv run pytest
uv run pytest -q
uv run pytest -q --model Bielik-11B-v3.0-Instruct
```

---

## Temperatura modeli

- Why5: domyÅ›lna temperatura `0.1` (patrz `agents_why5.py`)
- Ishikawa: domyÅ›lna temperatura `0.1` (patrz `agents_ishikawa.py`)
- Temperature-Check: domyÅ›lna temperatura `0.0` (patrz `agents_temperature_check.py`)

---

## Dodatkowe uwagi

- Parametr `--model` jest opcjonalny; jeÅ›li pominiÄ™ty, uÅ¼yty zostanie `MODEL` z env lub domyÅ›lny.
- Dla lokalnego endpointu (np. `llama-server`) wymagane jest uruchomienie serwera zgodnego z OpenAI API pod wskazanym adresem.

---

## Inferencja modelu Bielik (llama.cpp + GGUF)

Wszystkie przykÅ‚ady wykorzystujÄ… model [speakleash/Bielik-11B-v3.0-Instruct](https://huggingface.co/speakleash/Bielik-11B-v3.0-Instruct) oraz jego wariant GGUF [speakleash/Bielik-11B-v3.0-Instruct-GGUF](https://huggingface.co/speakleash/Bielik-11B-v3.0-Instruct-GGUF). Model moÅ¼e wymagaÄ‡ zalogowania w Hugging Face i akceptacji licencji.

### Krok 1: Przygotuj dostÄ™p do Hugging Face

- Zaloguj siÄ™ w `huggingface-cli login` lub ustaw zmiennÄ… `HF_TOKEN` z tokenem majÄ…cym dostÄ™p do modelu.
- Zaakceptuj warunki korzystania na karcie modelu (jeÅ›li wymagane).

### Krok 2: Uruchom lokalny serwer llama.cpp

Zapoznaj siÄ™ ze szczegÃ³Å‚ami instalacji tutaj: [https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp). Po instalacji model moÅ¼e zostaÄ‡ pobrany automatycznie przy pierwszym uruchomieniu dziÄ™ki flagom `-hf`. PoniÅ¼sza komenda startuje `llama-server` z kwantyzacjÄ… Q8_0 i kontekstem 32768 tokenÃ³w:

```bash
llama-server --port 1234 -c 32768 \
    -hf speakleash/Bielik-11B-v3.0-Instruct-GGUF:Bielik-11B-v3.0-Instruct.Q8_0.gguf
```

Serwer OpenAI-compatible bÄ™dzie dostÄ™pny pod `http://localhost:1234/v1`. JeÅ›li zmienisz port lub kontekst, zaktualizuj odpowiednio parametr `-c` i zmiennÄ… `LOCAL_BASE_URL`.

### Krok 3: Integracja z DORSZ

1. Ustaw `LOCAL_BASE_URL=http://localhost:1234/v1` (np. w `.env`).
2. (Opcjonalnie) Ustaw `MODEL=Bielik-11B-v3.0-Instruct`, aby nie podawaÄ‡ go w CLI.
3. Uruchom dowolnego agenta, wskazujÄ…c model Bielik-11B-v3.0-Instruct:

```bash
uv run python main.py why5 --provider local --model Bielik-11B-v3.0-Instruct
uv run python main.py ishikawa --provider local --model Bielik-11B-v3.0-Instruct
uv run python main.py temperature_check --provider local --model Bielik-11B-v3.0-Instruct
```

To wszystko â€“ jedynym wymaganym komponentem do inferencji jest `llama.cpp` z automatycznym pobieraniem GGUF.
