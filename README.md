# DORSZ - DokÅ‚adne Odpytywanie RozpoznajÄ…ce Sedno Zagadnienia

Uniwersalny runtime dla wielu agentÃ³w AI (obecnie: Why5-Ishikawa oraz Temperature-Check) korzystajÄ…cych z interfejsu OpenAI-compatible. Projekt jest nastawiony na dewelopera Pythona: uÅ¼ywa uv do zarzÄ…dzania Å›rodowiskiem i zaleÅ¼noÅ›ciami, a uruchamianie odbywa siÄ™ przez `uv run`.

## Szybki start

1) Wymagania:
- Python >= 3.13
- [uv](https://github.com/astral-sh/uv)
- Uruchomiony provider LLM (co najmniej jeden z): LMS Studio (LM Studio), Ollama lub OpenAI

2) Instalacja uv:
- macOS (Homebrew):
```bash
brew install uv
```
- Skrypt instalacyjny (macOS/Linux):
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

3) Klon repozytorium i instalacja zaleÅ¼noÅ›ci:
```bash
git clone <repo-url>
cd dorsz
uv sync
```

4) Uruchom pierwszy agent (przykÅ‚ad z LMS Studio i modelem Bielik):
```bash
uv run python main.py why5_ishikawa --provider lms --model bielik-11b-v2.6-instruct
```

Uwaga:
- Dla LMS Studio (LM Studio) wÅ‚Ä…cz endpoint OpenAI-compatible na `http://localhost:1234/v1`.
- Dla Ollama upewnij siÄ™, Å¼e serwer dziaÅ‚a na `http://localhost:11434/v1` i masz pobrany model (`ollama pull ...`).
- Dla OpenAI ustaw `OPENAI_API_KEY` w Å›rodowisku lub pliku `.env`.

---

## Uruchamianie â€” skÅ‚adnia

OgÃ³lny wzorzec:
```bash
uv run python main.py <agent_id> --provider <lms|ollama|openai> --model <nazwa_modelu>
```

Parametry:
- `agent_id` (wymagany): `why5_ishikawa` | `temperature_check`
- `--provider` (opcjonalny): `lms` (domyÅ›lny), `ollama`, `openai`
- `--model` (wymagany): nazwa modelu zgodna z wybranym providerem

PrzykÅ‚ady (z uÅ¼yciem modelu Bielik oraz innych):

- Why5-Ishikawa:
```bash
# LMS Studio + Bielik
uv run python main.py why5_ishikawa --provider lms --model bielik-11b-v2.6-instruct

# Ollama (przykÅ‚adowy model)
uv run python main.py why5_ishikawa --provider ollama --model llama3.2

# OpenAI (wymaga OPENAI_API_KEY)
uv run python main.py why5_ishikawa --provider openai --model gpt-4o
```

- Temperature-Check:
```bash
# LMS Studio + Bielik
uv run python main.py temperature_check --provider lms --model bielik-11b-v2.6-instruct

# Ollama (przykÅ‚adowy model)
uv run python main.py temperature_check --provider ollama --model llama3.2

# OpenAI (wymaga OPENAI_API_KEY)
uv run python main.py temperature_check --provider openai --model gpt-4o
```

WskazÃ³wka: domyÅ›lne wejÅ›cie poczÄ…tkowe dla kaÅ¼dego agenta jest ustawione w `main.py` w `AGENT_DEFAULT_INPUTS`.

---

## Agenci

Projekt zawiera kilka agentÃ³w. Opis kaÅ¼dego z nich znajduje siÄ™ w dedykowanym podrozdziale.

### 1) Agent Why5-Ishikawa

Asystent AI do analizy przyczyn ÅºrÃ³dÅ‚owych, Å‚Ä…czÄ…cy:
- MetodÄ™ â€5 Dlaczegoâ€ â€” iteracyjne zadawanie â€dlaczego?â€, aby odkryÄ‡ prawdziwe przyczyny ÅºrÃ³dÅ‚owe (root causes)
- Diagram Ishikawy (5M+E) â€” klasyfikacja przyczyn wedÅ‚ug kategorii:
  - CzÅ‚owiek (Man)
  - Maszyna (Machine)
  - MateriaÅ‚ (Material)
  - Metoda (Method)
  - ZarzÄ…dzanie (Management)
  - Åšrodowisko (Environment)

Jak dziaÅ‚a:
1. Agent rozpoczyna rozmowÄ™, pytajÄ…c o problem do przeanalizowania (uÅ¼ywa narzÄ™dzia `ask_human`).
2. Zadaje proste pytania â€Dlaczego?â€, schodzÄ…c coraz gÅ‚Ä™biej (5â€“15 iteracji).
3. Eksploruje rÃ³Å¼ne Å›cieÅ¼ki przyczynowe â€” jeÅ›li uÅ¼ytkownik utknie, bada inne aspekty.
4. Generuje strukturalne podsumowanie: sformuÅ‚owanie problemu, listÄ™ przyczyn z kategoriami Ishikawy, dziaÅ‚ania naprawcze, kluczowe wnioski.

Strukturalny output (Pydantic): zdefiniowany w `agents_why5_ishikawa.py` jako `Why5IshikawaSummary`.

PrzykÅ‚ad uruchomienia z Bielik:
```bash
uv run python main.py why5_ishikawa --provider lms --model bielik-11b-v2.6-instruct
```

PrzykÅ‚adowa sesja (fragment):
```
DORSZ - DokÅ‚adne Odpytywanie RozpoznajÄ…ce Sedno Zagadnienia
Provider: lms
Model: bielik-11b-v2.6-instruct
Agent: why5_ishikawa
Wpisz swÃ³j problem lub pytanie, aby rozpoczÄ…Ä‡ analizÄ™.

ğŸ¤– Agent: CzeÅ›Ä‡! ChÄ™tnie pomogÄ™ Ci przeanalizowaÄ‡ jakiÅ› problem metodÄ… "5 Dlaczego"
i diagram Ishikawy. Jaki problem lub sytuacjÄ™ chciaÅ‚byÅ› przeanalizowaÄ‡?

ğŸ¤” Agent pytanie: Jaki problem chciaÅ‚byÅ› przeanalizowaÄ‡?
ğŸ‘¤ Twoja odpowiedÅº: ZespÃ³Å‚ nie dotrzymuje deadlineÃ³w w projektach

[... dalsza interaktywna rozmowa ...]

================================================================================
ğŸ¦… PODSUMOWANIE ANALIZY '5 DLACZEGO' + ISHIKAWA
================================================================================
ğŸ“‹ Problem: ZespÃ³Å‚ regularnie nie dotrzymuje deadlineÃ³w w projektach...
ğŸ” Odkryte przyczyny ÅºrÃ³dÅ‚owe (â€¦)
âš¡ DziaÅ‚ania naprawcze (â€¦)
ğŸ’¡ Kluczowe wnioski (â€¦)
================================================================================
```

### 2) Agent Temperature-Check

Prosty agent testowy, prezentujÄ…cy wywoÅ‚anie narzÄ™dzia i zwracanie wynikÃ³w w strukturze.

Zasady dziaÅ‚ania:
- Ma dedykowane narzÄ™dzie `get_temperature(place)`, ktÃ³re w tym scenariuszu zwraca staÅ‚Ä… odpowiedÅº (do demonstracji integracji tool-call).
- Zawsze wywoÅ‚uje narzÄ™dzie dokÅ‚adnie raz.
- Zwraca wynik jako strukturÄ™ `TemperatureReport` (Pydantic) z polami: miejsce, temperatura (Â°C/Â°F), warunki.

DomyÅ›lne wejÅ›cie:
- JeÅ›li nie podano miejsca, wykorzystywane jest â€Warszawaâ€.
- W aktualnym runtime wejÅ›cie poczÄ…tkowe jest dostarczane z `AGENT_DEFAULT_INPUTS` (patrz `main.py`). Aby zmieniÄ‡ domyÅ›lne miejsce startowe, zaktualizuj `AGENT_DEFAULT_INPUTS["temperature_check"]`.

PrzykÅ‚ad uruchomienia z Bielik:
```bash
uv run python main.py temperature_check --provider lms --model bielik-11b-v2.6-instruct
```

---

## Konfiguracja providerÃ³w

DomyÅ›lne adresy bazowe (zdefiniowane w `main.py`):
- LMS Studio: `http://localhost:1234/v1`
- Ollama: `http://localhost:11434/v1`
- OpenAI: uÅ¼ywa domyÅ›lnych ustawieÅ„ biblioteki OpenAI (wymaga `OPENAI_API_KEY`)

Zmienna `--provider` przyjmuje wartoÅ›ci: `lms` (domyÅ›lny), `ollama`, `openai`.

---

## Struktura kodu

- `main.py` â€” ogÃ³lny runtime:
  - Rejestr agentÃ³w, fabryki (`AGENT_FACTORIES`) i renderery (`AGENT_RENDERERS`)
  - DomyÅ›lne wejÅ›cia startowe (`AGENT_DEFAULT_INPUTS`)
  - Konfiguracja providerÃ³w i klienta OpenAI-compatible
  - PÄ™tla uruchomieniowa i drukowanie wynikÃ³w
- `agents_why5_ishikawa.py` â€” definicja agenta Why5-Ishikawa:
  - Instrukcje, narzÄ™dzie `ask_human`, modele Pydantic, renderer podsumowania
- `agents_temperature_check.py` â€” definicja agenta Temperature-Check:
  - NarzÄ™dzie `get_temperature`, struktura `TemperatureReport`, renderer

---

## Konfiguracja (.env)

Plik `.env` (opcjonalny), wczytywany automatycznie:
```bash
# Dla OpenAI:
OPENAI_API_KEY=sk-...

# Opcjonalnie: inne ustawienia Å›rodowiskowe
```

---

## Temperatura modeli

- Why5-Ishikawa: domyÅ›lna temperatura `0.1` (patrz `agents_why5_ishikawa.py`)
- Temperature-Check: domyÅ›lna temperatura `0.0` (patrz `agents_temperature_check.py`)

---

## Dodatkowe uwagi

- WybÃ³r modelu (`--model`) jest wymagany dla wszystkich providerÃ³w.
- Dla LMS Studio i Ollama wymagane jest uruchomienie lokalnego serwera zgodnego z OpenAI API pod wskazanymi adresami.

